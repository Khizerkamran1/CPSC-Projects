---
title: "DATA 603 Khizer Kamran 30038809 Assignment 2"
author: "Khizer Kamran"
date: "2022-11-18"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

Problem 1

Load the Problem 1 Data

```{r}
data_p1 = read.csv("C:/Users/khizz/OneDrive/Desktop/Masters of Data Science and Analytics (MDSA)/Fall 2022/DATA 603/Assignments/Assignment #2/tires.csv") #Read in the tires.csv data file
```

Part A

1) T-test for Coefficients

```{r}
reg1 <- lm(wear ~ ave + type, data = data_p1) #Create the regression model
print(summary(reg1)) #Find the coefficient values
```

2) Interpretation

We can evaluate here that since all 2 of our independent variables have 
p-values of 2 x 10 ^ -16 (OR ~ 0), that both predictors hold significance
and thus must be included within our estimated best fit model.

3) Estimated Best Fit Model

NOTE: The variable names are used to represent X1, X2,.., Xn 
(Not Applicable to the Intercept)

f(wear) = [0.0113094 * (ave)] + [0.1725006 * (typeb)] - 0.6445083

Part B

1) Define the Dummy Variable

```{r}
reg1 <- lm(wear ~ factor(type), data = data_p1) #Create the regression model 
                                              #with 
                                              #the dummy variable
print(summary(reg1)) #Find the coefficient values
```

2) Interpret the Dummy Variables (Binaries)

Tire B - Code 0 [0.22000]
 
Tire A - Code 1 [0.72000]


Part C

β0 (Intercept) can be interpreted as the average tread wear 
among tire A type.

β1 as the average difference in average tread wear between both type A 
and type B tired types.

β0 + β1 can be interpreted as the average tread wear among tire B type.

Part D

1) Hypothesis Test

H0: βi=0
Ha: βi≠0 (i=1,2,...,p)

2) Interaction Models

```{r}
library(equatiomatic)
reg2 <- lm(wear ~ (ave + type) ^ 2, data = data_p1) #Use interactions
extract_eq(reg2)
print(summary(reg2)) #Find the coefficient values
```

3) Interpretation

As you can see from the output, with the p-value of 2.2 x 10 ^ -16, indicating 
that we should clearly reject the null hypothesis which means that we should 
definitely add the interaction term to the model at α = 0.05. Moreover, R2adj = 
0.96, means that 96.00% of the variation of the response variable is explained 
by the interaction model compared to only 0.8844 (88.44%) for the additive model 
that predicts 'wear' using all variables without an interaction term.

wear= -1.4689 + (-1.0800 x dummy) + (0.0208 x ave) + (-0.0120 x ave x dummy)

Part E

1) Rsqr.adj Values

```{r}
library(equatiomatic)
reg2 <- lm(wear ~ (ave + type) ^ 2, data = data_p1) #Use interactions
extract_eq(reg2)
summary(reg2)$adj.r.squared #Find the R ^ 2 adjusted of the full model
```

2) Interpretation

As you can see from the output, with the p-value of 2.2 x 10 ^ -16, indicating 
that we should clearly reject the null hypothesis which means that we should 
definitely add the interaction term to the model at α = 0.05. Moreover, R2adj = 
0.96, means that 96.00% of the variation of the response variable is explained 
by the interaction model (with all independent variables - type and ave - 
included).

Part F

1) Calculate the Tire Wear

```{r}
library(equatiomatic)

reduced <- lm(wear ~ (ave + type) ^ 2, data = data_p1) #Use interactions

extract_eq(reduced)

new_reduced <- data.frame(ave = 100, type = "A") #Given values to help 
#calculate wear

predict(reduced, new_reduced, interval = "predict") #Predict the value 
#of the wear
```

2) Interpret the Tire Wear

For every 160 KM the Tire Wears down by 0.49 in the context of tread 
thickness.

Problem 2

Load the Problem 2 Data

```{r}
data_p2 = read.csv("C:/Users/khizz/OneDrive/Desktop/Masters of Data Science and Analytics (MDSA)/Fall 2022/DATA 603/Assignments/Assignment #2/MentalHealth.csv") #Read in the MentalHealth.csv data file
```

Part A

1) Dependent Variables

The dependent variable is: EFFECT

Part B

1) Independent Variables

The independent variables are: AGE and METHOD

Part C

1) Scatter Diagram

```{r}
library(ggplot2) #Load the ggplot library for graphing

ggplot(data=data_p2,mapping= aes(x=AGE,y=EFFECT,colour=METHOD, shape=METHOD))+ 
  #Graph the SPs
ggtitle("Distribution of Effects Across Age Using Different Methods")+
geom_point()  #Add the scatter plot with the title and functions
```
2) Interpretation of Scatter Diagram

The scatter diagram displays the relationship between AGE and EFFECT, whilst
being categorized by the METHODs used (indicated by shape and color of the 
data points). We can see from the graph that the data illustrates varying 
levels of positive correlations between the AGE and EFFECT variables when
distributed by the METHOD types of A, B, and C. The slopes all indicate 
positive correlation; however, we can see that METHOD C has the steepest 
slope (indicating a very strong positive correlation between AGE and EFFECT)
where as METHOD B indicates a moderately strong positive correlation 
between AGE and EFFECT, and lastly METHOD C which indicates a slightly lower
yet still positive correlation between AGE and EFFECT.

Part D

1) Hypothesis Test

Ho:βi=0
HA:βi≠0 (i=1,2,...,p)

2) Test the Interaction Terms

```{r}
library(equatiomatic)
mixmodel<- lm(EFFECT~(AGE+factor(METHOD))^2,data=data_p2)
#Create the mixed regression model; removing limitations
extract_eq(mixmodel)
summary(mixmodel)
```

3) Interpretation

As you can see from the output, tcal= 4.056 with the p-value (0.000328 < 0.05), 
indicating that we should clearly reject the null hypothesis which means 
that the variable AGE has a significant influence or interaction with the 
EFFECT dependent variable, at α = 0.05. Likewise the 3 '***' indicates a
highly significant p-value for the AGE predictor in relation to the EFFECT
variable.

Part E

1) Final Sub-models

NOTE: x <=> AGE

EFFECT(method_a) = 47.51559 + (0.33051 * x) 
EFFECT(method_b) = 47.51559 + (0.33051 * x) + (-18.59739) + (0.19318 * x)
EFFECT(method_c) = 47.51559 + (0.33051 * x) + (-41.30421) + (0.70288 * x)

NOTE: Sub-models When Simplified

EFFECT(method_a) = 47.5 + (0.33 * x) 
EFFECT(method_b) = 28.9 + (0.52 * x)
EFFECT(method_c) = 6.21 + (1.03 * x)

Part F

1) Interpretation of the Final Sub-models

For patients in this study receiving treatment A, the effectiveness of the 
treatment is predicted to increase by 0.33 units for every additional year 
in age.

For patients in this study receiving treatment B, the effectiveness of the 
treatment is predicted to increase by 0.52 units for every additional year 
in age.

For patients in this study receiving treatment C, the effectiveness of the 
treatment is predicted to increase by 1.03 units for every additional year 
in age.

Part G

1) Scatter Diagram (With Regression Lines)

```{r}
library(ggplot2) #Load the ggplot library for graphing

mixmodel<- lm(EFFECT~AGE+factor(METHOD)+(AGE*factor(METHOD)),data=data_p2)
#Create the mixed regression model; removing limitations
summary(mixmodel) #Get the summary statistics

method_a=function(x){47.51559+(0.33051*x)} #Create the functions for A/B/C
method_b=function(x){47.51559+(0.33051*x)+(-18.59739)+(0.19318*x)}
method_c=function(x){47.51559+(0.33051*x)+(-41.30421)+(0.70288*x)}
ggplot(data=data_p2,mapping= aes(x=AGE,y=EFFECT,colour=METHOD,shape=METHOD))+ 
  #Graph the SPs
ggtitle("Distribution of Effects Across Age Using Different Methods")+
geom_point()+  #Add the scatter plot with the title and functions
  stat_function(fun=method_a,geom="line",color=scales::hue_pal()(2)[1])+
  stat_function(fun=method_b,geom="line",color=scales::hue_pal()(3)[2])+
  stat_function(fun=method_c,geom="line",color=scales::hue_pal()(2)[2])
```

2) Interpretation of Scatter Diagram (With Regression Lines)

We have three different regression lines for METHOD A/B/C. But now those 
regression lines have different intercepts, β0+β2 versus β1, as well as 
different slopes, β1+β3 versus β1. 

This allows for the possibility that changes in AGE may affect the EFFECT 
the mental health of each patient differently (depending on the 
METHOD used A/B/C). The output shows the estimated relationships between 
AGE and EFFECT for each METHOD utilized to treat their mental health, within 
the model. We note that the slope for METHOD A (0.33051) is lower than the 
slope for METHOD B (0.52359), which is lower than the slope for 
METHOD C (1.03339). This suggests that increases in AGE are 
associated with smaller increases in EFFECT with METHOD A, moderately higher 
increases with the usage of METHOD B, and significantly higher influences with
the usage of METHOD C.

This interpretation is consistent from our conclusions in Part F, as the 
regression lines indicate the linearity of the slopes but also the impact
these slops will have on the AGE variable as the AGE variable
increases by 1 unit, that is:

For patients in this study receiving treatment A, the effectiveness of the 
treatment is predicted to increase by 0.33 units for every additional year 
in age. (Whilst also being slightly-to-moderately positively correlated).

For patients in this study receiving treatment B, the effectiveness of the 
treatment is predicted to increase by 0.52 units for every additional year 
in age. (Whilst also being moderately positively correlated).

For patients in this study receiving treatment C, the effectiveness of the 
treatment is predicted to increase by 1.03 units for every additional year 
in age. (Whilst also being significantly positively correlated).

Problem 3

Load the Problem 3 Data

```{r}
data_p3 = read.table("FLAG2.txt", header = TRUE,  sep ="\t") #Read in the FLAG2.txt data file
```

Part A

1) Stepwise Regression Procedure

```{r}
library(olsrr)#need to install the package olsrr

fullmodel<-lm(LOWBID ~ DOTEST + factor(STATUS) + factor(DISTRICT) + NUMIDS + 
                DAYSEST +
                RDLNGTH + PCTASPH + PCTBASE + PCTEXCAV + PCTMOBIL +
                PCTSTRUC + PCTTRAF, data = data_p3) #Create the full model with
                                                    #factors

summary(fullmodel) #Summary statistics

stepmod=ols_step_both_p(fullmodel,pent = 0.05, prem = 0.1, details=TRUE)
#Stepwise Regression Procedure

summary(stepmod$model) #Summary of the regression procedure
```
2) Identify the Correct Model

f(LOWBID) = 5.711e+04 + 9.374e-01(DOTEST) + 9.525e+04(factor(STATUS)1)
        - 1.535e+04(NUMIDS)

Part B

1) Forward Regression Procedure

```{r}
library(olsrr)#need to install the package olsrr

fullmodel<-lm(LOWBID ~ DOTEST + factor(STATUS) + factor(DISTRICT) + NUMIDS + 
                DAYSEST +
                RDLNGTH + PCTASPH + PCTBASE + PCTEXCAV + PCTMOBIL +
                PCTSTRUC + PCTTRAF, data = data_p3) #Create the full model with
                                                    #factors

summary(fullmodel) #Summary statistics

formodel=ols_step_forward_p(fullmodel,penter = 0.05, details=TRUE)
#Stepwise Regression Procedure

summary(formodel$model) #Summary of the regression procedure
```

2) Identify the Correct Model

f(LOWBID) = 5.711e+04 + 9.374e-01(DOTEST) + 9.525e+04(factor(STATUS)1)
        - 1.535e+04(NUMIDS)

Part C

1) Forward Regression Procedure

```{r}
library(olsrr)#need to install the package olsrr

fullmodel<-lm(LOWBID ~ DOTEST + factor(STATUS) + factor(DISTRICT) + NUMIDS + 
                DAYSEST +
                RDLNGTH + PCTASPH + PCTBASE + PCTEXCAV + PCTMOBIL +
                PCTSTRUC + PCTTRAF, data = data_p3) #Create the full model with
                                                    #factors

summary(fullmodel) #Summary statistics

backmodel=ols_step_backward_p(fullmodel, prem = 0.05, details=TRUE)
summary(backmodel$model)
```

2) Identify the Correct Model

f(LOWBID) = 5.711e+04 + 9.374e-01(DOTEST) + 9.525e+04(factor(STATUS)1)
        - 1.535e+04(NUMIDS)

Part D

1) Individual t-test

```{r}
fullmodel<-lm(LOWBID ~ DOTEST + factor(STATUS) + factor(DISTRICT) + NUMIDS + 
                DAYSEST +
                RDLNGTH + PCTASPH + PCTBASE + PCTEXCAV + PCTMOBIL +
                PCTSTRUC + PCTTRAF, data = data_p3) #Create the full model with
                                                    #factors
summary(fullmodel)
sigma(fullmodel)
```
2) Interpet the Results of the t-test

Significant Predictors:

1. DOTEST (t-cal = 55.494, p-value = <2e-16)
2. factor(STATUS)1 (t-cal = 2.554, p-value = 0.0112)
3. factor(DISTRICT)4 (t-cal = -1.982, p-value = 0.0485)
4. NUMIDS (t-cal = -2.550, p-value = 0.0114)

From the example, the output shows that the aforementioned variables 
with the p-values < 0.05,indicating that we should clearly reject the 
null hypothesis that these predictors have significant influence on 
LOWBID at α = 0.05.

Part E

1) Compare Results from 3A - 3D

We know from the results above that 3A, 3B, and 3C provides us with the same
additive model (that only includes the intercept, DOTEST, factor(status)1, and
NUMIDS). From between 3A to 3D we have seen 3 primary independent variables that
have consistently been proven to be significant in influencing the 
LOWBID dependent variable. These variables include the following:

Consistent Predictors From 3A - 3D:

1. DOTEST 
2. factor(STATUS)1 
3. NUMIDS

We know from the results above that 3D provides us with a different
additive model (that includes the intercept, DOTEST, factor(status)1, 
factor(district)4, and NUMIDS). From between 3A to 3D we have seen 3 primary 
independent variables that have consistently been proven to be significant in 
influencing the LOWBID dependent variable. However, the inclusion of the new 
predictor (found below) changes our additive model:

New Predictors From 3D:

4. factor(DISTRICT)4

2) All Possible Additive Models

Additive Model #1:

f(LOWBID) = 5.711e+04 + 9.374e-01(DOTEST) + 9.525e+04(factor(STATUS)1)
        - 1.535e+04(NUMIDS)
        
Additive Model #2:

f(LOWBID) = 5.711e+04 + 9.374e-01(DOTEST) + 9.525e+04(factor(STATUS)1)
        - 2.729e+05(factor(DISTRICT)4) - 1.535e+04(NUMIDS)        

Part F

1) Calculate Absolute Difference Between District 1 and 4

```{r}
fullmodel<-lm(LOWBID ~ DOTEST + factor(STATUS) + factor(DISTRICT) + NUMIDS, 
              data = data_p3) #Create the full model with factors
                                                    
#summary(fullmodel)

d4 = 6.050e+04 + (-3.165e+05)
d1 = 6.050e+04
abs(d4 - d1)
```
Part G

1) Calculate Absolute Difference Between District 2 and 5

```{r}
fullmodel<-lm(LOWBID ~ DOTEST + factor(STATUS) + factor(DISTRICT) + NUMIDS, 
              data = data_p3) #Create the full model with factors
                                                    
#summary(fullmodel)

d5 = 6.050e+04 + (-1.415e+04)
d2 = 6.050e+04 + (7.100e+04)
abs(d5 - d2)
```

Part H

1) Conduct Individual t-test

```{r}
library(equatiomatic)
fullmodel<-lm(LOWBID ~ (DOTEST + factor(STATUS) + factor(DISTRICT) + NUMIDS)^2, 
              data = data_p3) #Create the full model with factors

extract_eq(fullmodel)
                                                    
summary(fullmodel)
```
2) Interpret the Results of the t-test

Significant Predictors:

1. DOTEST (tcal = 36.955  p-value = < 2e-16)
2. factor(DISTRICT)4 (tcal = -2.332, p-value = 0.02046) 
3. DOTEST:factor(STATUS)1 (t-cal = 2.573, p-value = 0.01063)
4. DOTEST:factor(DISTRICT)5 (t-cal = -4.636, p-value = 5.64e-06)
5. DOTEST:NUMIDS (t-cal = -5.367, p-value = 1.77e-07)
6. factor(DISTRICT)4:NUMIDS (t-cal = 3.260, p-value = 0.00126)

From the example, the output shows that the aforementioned variables 
with the p-values < 0.05,indicating that we should clearly reject the 
null hypothesis and that these predictors indeed have significant influence on 
LOWBID at α = 0.05. With the p-value < 0.0001, this also indicates that we 
should clearly reject the null hypothesis which means that we should definetely 
add the interaction term to the model at α = 0.05. 

3) Final Model

f(LOWBID) = -3.353e+04 + 1.097e+00(DOTEST) - 1.532e+06(factor(DISTRICT)4)
  + 9.451e-02(DOTEST * factor(STATUS)1) - 1.330e-01(DOTEST * factor(DISTRICT)5)
  - 1.934e-02(DOTEST * NUMIDS) + 1.519e+05 (factor(DISTRICT)4 * NUMIDS)
  
Part I

1) RMSE Value From Part H's Model

```{r}
library(equatiomatic)
fullmodel<-lm(LOWBID ~ (DOTEST + factor(STATUS) + factor(DISTRICT) + NUMIDS)^2, 
              data = data_p3) #Create the full model with factors
extract_eq(fullmodel)
sigma(fullmodel)
```
The calculated RMSE value from the above model is:

Part I's RMSE = 251811.6

Part D's RMSE = 277966.8

We can compare and evaluate that Part I results in a much lower RMSE value. 
When comparing the values associated for the RMSEs among Part I and D, it is
evident that the better fit model is that of Part I.

2) Interpret the RMSE Value

We can conclude that the model from Part I fits the data better than 
the multiple linear regression model found in Part D. The RMSE values can be 
interpreted as the standard deviation of the unexplained variance, and has 
the useful property of being in the same units as the response variable. 

Part J

1) R ^ 2 Adjusted 

```{r}
library(equatiomatic)
fullmodel<-lm(LOWBID ~ (DOTEST + factor(STATUS) + factor(DISTRICT) + NUMIDS)^2, 
              data = data_p3) #Create the full model with factors

extract_eq(fullmodel)
summary(fullmodel)$adj.r.squared
```

2) Interpret the R ^ 2 Adjusted

Moreover, R2adj = 0.9809329, means that 98.09% of the variation of the response 
variable is explained by the interaction model, compared to only 0.9765 for the 
additive model that predicts LOWBID using the 4 predictor variables without an 
interaction term.

Problem 4

Load the Problem 2 Data

```{r}
data_p4 = read.csv("C:/Users/khizz/OneDrive/Desktop/Masters of Data Science and Analytics (MDSA)/Fall 2022/DATA 603/Assignments/Assignment #2/KBI.csv") #Read in the MentalHealth.csv data file
```

Part A

1) Stepwise Regression Model (With Stepwise Selection)

```{r}
library(olsrr) #Need to install the package olsrr

fullmodel<-lm(BURDEN ~ CGAGE + CGINCOME + CGDUR + ADL + MEM + COG + SOCIALSU, 
              data = data_p4) #Create the full regression model

summary(fullmodel) #Get the summary statistics for the full model

stepmod = ols_step_both_p(fullmodel, pent = 0.1, prem = 0.3, details=TRUE)
#Create the stepwise regression model

summary(stepmod$model) #Get the summary statistics of the stepwise model
```

2) Select the Most Significant Predictors

1. MEM
2. SOCIALSU
3. CGDUR

Where each of the predictors' names are used to represent X1,X2,..,Xn

f(BURDEN) = 0.56612 * (MEM) - 0.49237 * (SOCIALSU) + 0.12168 * (CGDUR)

Part B

1) All Possible Regression Selection

```{r}
# Option 2

library(olsrr) #Need to install the package olsrr

firstordermodel<-lm(BURDEN ~ CGAGE + CGINCOME + CGDUR + ADL + MEM + COG + 
                      SOCIALSU, data = data_p4) #Create the first order model

ks = ols_step_best_subset(firstordermodel, details=TRUE) 
#Use the all step best selection method


library(leaps)

best.subset <- regsubsets(BURDEN ~ CGAGE + CGINCOME + CGDUR + ADL + MEM + COG + 
                      SOCIALSU, data = data_p4, nv=8 ) 

summary(best.subset)

reg.summary<-summary(best.subset)

# for the output interpretation
cp<-c(reg.summary$cp)
AdjustedR<-c(reg.summary$adjr2)
RMSE<-c(reg.summary$rss)
BIC<-c(reg.summary$bic)
cbind(cp,BIC,RMSE,AdjustedR)

par(mfrow=c(2,2)) # split the plotting panel into a 3 x 2 grid
plot(reg.summary$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "Cp")
plot(reg.summary$bic,type = "o",pch=10, xlab="Number of Variables",ylab= "BIC")
plot(reg.summary$adjr2,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2 Adj")
plot(reg.summary$rss,type = "o",pch=10, xlab="Number of Variables",ylab= "RMSE")
```
2) Select the Most Significant Predictors

We want to look for the model that balances the following:

1. A high Adjusted R-square
2. A low RMSE
3. A low AIC/BIC
4. The most approximate CP value

Model 5 Selected:

1. CGDUR 
2. ADL 
3. MEM 
4. SOCIALSU
5. CGAGE

```{r}
firstordermodel<-lm(BURDEN ~ CGDUR + CGAGE + ADL + MEM + SOCIALSU, 
                    data = data_p4)
summary(firstordermodel)
```

f(BURDEN) = 102.07454 + 0.49773 * (MEM) - 0.46361 * (SOCIALSU) 
    + 0.11361  * (CGDUR) + 0.09951 * (ADL) + 0.13826 * (CGAGE)


Part C

1) Compare Significant Predictors From 4A - 4B

Significant Predictors from 4A:

1. MEM
2. SOCIALSU
3. CGDUR

Significant Predictors from 4B:

Model 5 Selected:

1. CGDUR 
2. ADL 
3. MEM 
4. SOCIALSU
5. CGAGE

From here we can see that the variables MEM, SOCIALSU, and CGDUR are 
consistent between 4A and 4B. However, 4B also includes ADL and CGAGE as a 
significant predictor (due to our assumptions of maintaining a balance
between the highest Adjusted R-square, the lowest possible AIC/BIC, 
the most approximate Cp, and the lowest RMSE values).

2) First Order Model With Interaction Terms

H0: βi=0
HA: βi≠0 (i=1,2,...,p)

```{r}
library(equatiomatic)
firstordermodel<-lm(BURDEN ~ (CGDUR + CGAGE + ADL + MEM + COG + 
                      SOCIALSU)^2, data = data_p4)
                     #Create the first order model with interaction terms

extract_eq(firstordermodel)
summary(firstordermodel)
```
    
3) Interpret the Results 

As you can see from the output, tcal= 4.729 with the p-value < 0.0001, 
indicating that we should clearly reject the null hypothesis which means 
that we should definitely add the interaction terms to the model at α = 0.05. 
Moreover, we should specifically add CGDUR, CGAGE, MEM, CGDUR * CGAGE, and ADL *
MEM as these p-values are < 0.05 and indicate that they significantly
influence the dependent variable BURDEN.

f(BURDEN) = -4.436e+01 + 4.537e+00 * (CGAGE) + 3.452e+00 * (MEM) 
    + 2.419e-02 * (CGDUR * CGAGE) - 2.001e-02 * (ADL * MEM)
